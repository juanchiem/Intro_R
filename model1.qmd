---
title: "Untitled"
format: html
---

## Outline

-   Why multiple linear regression?
-   Recap: simple linear regression basics
-   Case study: data exploration
-   The General Linear Model
-   Parameter estimation
-   Model checking
-   Inferences about model and parameters
-   Multicollinearity
-   Building the model
-   Predictions

```{r, eval = F}
pacman::p_load(tidyverse, rio, GGally, car, ggpmisc, gtable)
```

## Why multiple linear regression (MLR)?

We kown that biological processes are complex by nature. They involve many variables and complex interactions. Although SLR models are useful in some scenarios, oftenly they fail at modeling this problems.

- MLR allows us building **more realistic** and **interpretable** models.

As a result we need to build more realistic models. But at the same time, it is desirable to keep them interpatable. MLR can help us in this task.

Although it has a number of applications, they fall into broad categories. For example, MLR is used as a tool for 

- **Modeling** crop yield response to fertilizer rates

- **Predicting** crop yields based on weather, soil, management variables

- Identifying variables or factors that **explain** crop yields

- **Controlling variation** in designed experiments using quantiative covariables (ANCOVA)

- Estimating soil properties from easy to measure soil variables and pedotransfer functions (**variable substitution**)

## Recap: the linear model


$$\underbrace{Y}_{\substack{\text{random} \\ \text{variable}}} = f(X)=\underbrace{\beta_0 + \beta_1 X}_{\text{systematic}} + \underbrace{\varepsilon}_{\text{random}}$$


In the previous class we discussed the simplest type of regression model used to relate a response variable Y, also called dependent varaible, to a explanatory or independent variable X. Both variables were quantitative.

Although very simple, the model is very restrictive and could have high bias.


![](fig_3/slr.png) 

This model has 2 parameters, the intercept and the slope, the first one represents the value of the response when the X is 0. The slope represents the change of Y for each unit change of X. If slope is 0, then there is no relation between X and Y.


Assumptions

- $X$ is **fixed** (Model 1) or **random** (model 2).

- $Y$ is **random** with $E(Y|X)=\beta_0+\beta_1X$

- Errors are **independent** random variables with **normal distribution** 

$$e_\text{iid} \sim N(0,\sigma^2_\varepsilon) \\
\text{Cov}(e_i, e_j) = 0$$

This model has some assumptions.

![](fig_3/slr_assump.png) 

The straight line sumarize the linear relationship between expectation of Y a each value of X. We also see that each observation is a realization from a random variable with normal distribution with expectation conditioned by X and constant variance

Well see that these assumptions are still valid for MLR

## The study case: diploid wheat

![](fig_3/Triticum_monococcum0.jpg) 

Several morphological traits were measured for 190 seeds selected at random from a line of diploid wheat (*Triticum monococcum*). [(Jing et al.,  2007)](http://www.wgin.org.uk/information/documents/WGIN%20publications%20pdfs/Jing%202007.pdf)

The goal was to *identifying variables associated with differences in seed weight* 

```{r, echo = F}
triticum <- rio::import("data/triticum.xlsx")

triticum %>% 
  head() %>%  
  gt()
```

The variables measured were: `weight` (mg), `diameter` (mm), `length` (mm), `moisture` content (%), and endosperm `hardness` (single-kernel characterization system index value)

First things first, let's explore the data...

```{r splom, message=FALSE, out.width="80%", eval = F}
triticum %>% 
  select(-ID)  %>% 
  ggpairs() +
  theme_bw()
```

¿Are there any correlations between variables?

## The General Linear Model

$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p + \varepsilon$$

- $Y$ is the response or dependent variable
- $X$s are the $p$ explanatory or independent variables
- $\beta$s are the $p+1$ model coefficients or parameters
- $\varepsilon$ is the error term 

Assumptions:

- $X$s are related linearly to $Y$

- $\varepsilon \sim N(0, \sigma^2_\varepsilon)$

- $X$s are independent (no multicollineality)

This is the general linear model (GLM). In this case, the G does not mean Generalized.

It has the same assumptions of SLR and

$X$s **can be**:

  - quantitative variables, and its transformations
  - categorical variables as dummy 0/1 variables
  - cross-product terms of the same variables (powers), or different variables (interactions)
  
As its name says, it can accommodate both categorical and quantitative variables, some transformations and interactions (cross-products)

Following this we can see that the simple linear regression model relating two quantitative variables, or those models relating quantitative response to qualitative variables/factors (anova models) are special cases of it.

Although ideally predictors should be perfectly uncorrelated, in practice some correlation between then ocurr and can be handled. However, perfect functions between predictors are not allowed in the model.


_Why is it still called **linear**?_

Because $\beta$s are entered linearly (not as powers or products).
**The GLM is linear in the $\beta$'s**

Not linear in $\beta$s: 

$Y=\beta_0 X_1^{\beta_1} X_2^{\beta_2}+\varepsilon$

Still linear in $\beta$s: 

$Y=\beta_0+\beta_1X_1+\beta_2X_1^2+\varepsilon$ 

But why we call it linear if we can include powers, logs, etc. It is because of how the $\beta$' s are entered in the model rather than the variables.

## How the model works?

The simplest case: two quantitative predictors, no interactions

$$Y = \beta_0 + \beta_1X_1 + \beta_2 X_2 +\varepsilon $$

![](fig_3/3d_plane.png) 

The SLR represented the equation of a straight line in a 2D space, In the simplest case, 2 quant predictors, X1 and X2, the model gives the equation of a plane in a 3D space. 

- If no interactions:

  Helding $X_2$ constant, let's say $X_2 = x_2$ we have:

  $$Y_{X_2 = x_2} = (\beta_0 + \beta_2x_2) + \beta_1 X_1 + \varepsilon $$

  Only the intecept changes

If model doesnt include interactions we can se that if we hold X2 constant, its effect is added to the intercept and the model can be expressed as a function of X1. If we change the X2 value, only the intercept will change.

- If we had interactions...

  $$Y = \beta_0 + \beta_1X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \varepsilon $$

  Setting $X_2 = x_2$ we would have:

  $$Y_{X_2 = x_2} = (\beta_0 + \beta_2x_2) + (\beta_1 + \beta_3x_2)X_1 + \varepsilon  $$

Intercept and the slope of $X_1$ would change with $X_2$


In contrast, if the interaction is present, the same excersice would result in a model where both, the intercept and the slop, are functions of the X2. In other words, changing X2 would change how X1 relates to Y.


## Fitting the model

In a random sample, the *i*-th observation is:

$$y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \cdots + \beta_px_{ip} + \varepsilon_i$$

and the *i*-th fitted value is:

$$\hat{y}_i = \hat\beta_0 + \hat\beta_1x_{i1} + \hat\beta_2x_{i2} + \cdots + \hat\beta_px_{ip}$$


the are several methods for estimating the $\beta$'s, in this lecture we are going to cover the Ordinary Least-Squares (OLS)

It involves equating partial derivates to 0:

$$\dfrac{\partial \phi}{\partial \beta_0} = -2 \sum \left[ y_i - \left( \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}\right) \right] = 0 \\
\vdots \\
\dfrac{\partial \phi}{\partial \beta_p} = -2 \sum \left[ y_i - \left( \beta_1 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} \right) \right] (-x_{ip}) = 0$$

using matrix notation... how actually statistical packages get the computations done.



```{r, highlight.output = 11:14}
m <- lm(
  weight ~ length + diameter + hardness, 
  data = triticum
)
summary(m)
```


We can easily fit a multiple linear regression model using lm function. In this case no interactions are specified.

Then the summary() function provides the relevenat information for assesing the model as a whole and inspect the parameters

The fitted model:

$$\begin{align}
Y_\text{weight} &= -31.95 + (-38.25) X_\text{length} \\
& \quad + 87.98 X_\text{diameter}  + (-0.05)X_\text{hardness} 
\end{align}$$

_What these coefficients mean?_

- They are **partial slopes**: effect of changing an $X$ but keeping the rest constant.

- Unless standardized, they do not show their importance on the model.


Example: $X_\text{length} = -38.25$ means that for each unit increase in length and holding the remaining predictors constant, the weight would decrease 40.15 units

## Estimating the error of the model

Errors are the difference between observed $y_i$ and estimated or fitted values $\hat y_i$ :

$$\begin{align}
e_i &= y_i - \hat{y}_i \\
&= y_i - (\hat\beta_0 + \hat\beta_1x_{i1} + \hat\beta_2x_{i2} + \cdots + \hat\beta_px_{ip})
\end{align}$$

and the SSErr is:

$$\SSErr = \sum{(y_i - \hat{y}_i)^2}$$ 

The **mean squared error** (MSErr) is:

$$\MSErr = \dfrac{\SSErr}{n-p-1} = \dfrac{\sum{(y_i - \hat{y}_i)^2}}{n-p-1}$$
Finally the estimate of the **standard error of the model**:

$$\hat\sigma_\varepsilon = \RMSE = \sqrt{\dfrac{\SSErr}{n-p-1}} = \sqrt{\dfrac{\sum{(y_i - \hat{y}_i)^2}}{n-p-1}}$$

We can get it form our model by using `sigma()` function

```{r}
sigma(m)
```

_How to interpret this?_

- The lower, the better.

- It is the average deviation of a prediction from the observed values

- Empirical rule: 95% of the prediction errors fall $\pm$ 2 RMSE

- We can relate it to $\bar{Y}$

## Model checking

Looking at the residuals we can check if the model assumptions are supported

Inspecting what is left in the residuals can help us assess if the assumptions are met

```{r, out.width="70%"}
plot(m, which = 1)
```

¿Is there any pattern left in the residuals?

```{r, out.width="70%"}
plot(m, which = 2)
```

¿Are the errors normally distributed?

```{r, out.width="70%"}
plot(m, which = 3)
```

¿Are their variances constant?

```{r, out.width="70%"}
plot(m, which = 4)
```

¿Any leverage value?

This checking is about the model as a whole. For more insights about predictors, additional diagnostic plots are needed: partial residuals.

## Assessing the model


Applying the sums of squares partition from simple linear regression model 

$$\begin{array}{ccccc}
& \SSTot & = & \SSReg & + & \SSErr \\
& \sum \left(y_i - \bar{y} \right)^2 & = & \sum \left(\hat{y}_i - \bar{y} \right)^2 & + & \sum \left(y_i - \hat{y}_i \right)^2 \\
\text{d.f.} &n-1 & = & p & + & n-p-1
\end{array}$$

```{r, fig.height = 4.5, echo = F, include = F}
dummy <- data.frame(X = 0:10, Y = 3:13)
flecha <- arrow(ends="both", angle=90, length=unit(.2,"cm"))
ggplot(dummy, aes(X, Y)) +
  geom_line() +
  geom_hline(yintercept = 8, lty = 2) +
  labs(x = "X", y = "Y") +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 15)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 10)) +
  theme_bw(base_size = 16) + tema +
  annotate("point", x = 7, y = 13, colour = "red", size = 1.5) +
  annotate('segment', x = 7, xend = 7, y = 10, yend = 13, arrow = flecha) +
  annotate("text", x = 0, y = 8.5, label = "italic(bar(Y))", hjust = -0.5, parse = T) +
  annotate("text", x = 6.5, y = 12, label = "paste(italic(Y), ' - ', italic(hat(Y)))", hjust = -1, parse = T) +
  annotate('segment', x = 7, xend = 7, y = 8, yend = 10, arrow = flecha) + 
  annotate("text", x = 6.5, y = 9, label = "paste(italic(hat(Y)), ' - ', italic(bar(Y)))", hjust = -1, parse = T) +
  annotate("segment", x = 6.5, y = 8, xend = 6.5, yend = 13, arrow = flecha) +
  annotate("text", x = 6, y = 11, label = "paste(italic(Y), ' - ', italic(bar(Y)))", hjust = 1, parse = T)
```


$\SSReg$ summarizes how much variability of $Y$ is captured by the model as a **whole** having the intercept and $X_1,\dots,X_p$ as predictors. 

{{content}}

]

???

READ


--

¿How much $\SSReg$ is good enough?

{{content}}

--

We can use the **Coefficient of Determination** $R^2$

$$R^2 = \dfrac{\SSReg}{\SSTot} = 1-\dfrac{\SSErr}{\SSTot}$$


???

READ
--


.pull-right[

Overal hypothesis testing using ANOVA


$$H_0: \beta_1 = \beta_2 = \cdots = \beta_p = 0 \\ H_1: \text{at least one } \beta_j \ne 0$$


| Source           |  df     |  SS           | MS                      | $F_0$                    |
|------------------|:-------:|:-------------:|:-----------------------:|:-------------------------|
| Regression       | $p$     | $\fSSReg$     | $\SSReg$                | $\dfrac{\MSReg}{\MSErr}$ |
| Error            | $n-p-1$ | $\fSSErr$     | $\dfrac{\SSErr}{n-p-1}$ |                          |
| Total            | $n-1$   | $\fSSTot$     |                         |                          |


$F_0 = \dfrac{\MSReg}{\MSErr} \sim F_{p;n-p-1}$

Reject $H_0$ if: $P(F_0 > F) \le \alpha$

]

???

Describe ANOVA, mention the overall hipotesis


---
template: anova

.pull-left[

```{r, highlight.output = 18:20}
summary(m)
```

]

???

Describe the output, the SSQ

--

.pull-right[

- Enough evidence for rejecting $H_0$ (P < 0.001) thus the overall regression model is statistically significant.

- About 80% of the seed `weight` variability is explained by the model having `diameter`, `moistue`, and `hardness` as predictors

]

---
name: seqSS

## Predictors contribution

.pull-left[

We could sequentially partition $\SSReg$ to explore the contribution of each predictor. However...

- If $X$s are perfectly uncorrelated, sequential SS are **orthogonal**

  $$\SSReg = \text{SS}_{X_1} + \text{SS}_{X_2} + \cdots + \text{SS}_{X_p}$$
  
  Each SS reflects the **unique** contribution of each predictor 

- If not, SS are **not orthogonal** and sequential partition is not unique

  $$\SSReg = \text{SS}_{X_1} + \text{SS}_{X_2|X_1} + \cdots + \text{SS}_{X_p|X_1, X2,\cdots, X_{p-1}}$$

  Each SS reflects the contribution of each predictor in a model having the previous predictors in it.
  
  **Order matters!** 

]

???

Read

---
template: seqSS

.pull-right[

We can see this in the ANOVA Type I SS table (sequetinal)

`weight ~ length + diameter + hardness ` 

```{r, highlight.output = c(5,6)}
anova(m)
```

]

---
template: seqSS

.pull-right[

Changing the order

`weight ~ diameter + hardness + length` 

```{r, highlight.output = c(5,7)}
m2 <- update(m, . ~ diameter + hardness + length)
anova(m2)
```

]

---
template: seqSS

.pull-right[

An alternative, look at marginal SS (Type III) where order doesn't matter!:

$$\SSReg = \text{SS}_{X_1|X_2,\dots,X_p} + \text{SS}_{X_2|X_1,\dots,X_p} + \cdots + \text{SS}_{X_p|X_1, X2,\cdots, X_{p-1}}$$

```{r, highlight.output = c(6,8)}
Anova(m2, type = 3)
```

]

???

---
name: coef-tests

## Testing parameters

.pull-left[

The error associated with each parameter is:

$$\hat\sigma_{\hat\beta_j} = \hat\sigma_\varepsilon \sqrt{\dfrac{1}{\sum (x_{ij} - \bar{x}_j)^2(1-R^2_j)}}$$

$R^2_j$ is the $R^2$ from regressing $X_j$ agains the rest of $X$s

**Important:** higher uncertainty in $\hat\beta_j$s when

- The larger residual standard error, 
- The lesser variability in $X_j$
- The larger proportion of variance of $R^2_j$

**Hypothesis test** 

$H_0: \beta_j = 0 \quad \text{vs} \quad H_1: \beta_j \ne 0$

$t_{\beta_j} = \beta_j/\hat\sigma_{\beta_j} \sim t_{n-p-1}$, reject $H_0$ if $P(>|t|) < \alpha$


**Confidence intervals** 

$$\hat\beta_j \pm t_{1-\alpha/2, n-p-1} \hat{\sigma}_{\hat\beta_j}$$ 

]

--

.pull-right[

Coefficients and t-tests

```{r}
summary(m)$coefficients
```

95% Confidence Intervals

```{r}
confint(m)
```

]

---
## Multicollinearity

.pull-left[

It occurs when some degree of correlation between independent variables exists.

```{r}
triticum |>
  select(length, diameter, moisture, hardness) |> 
  cor()
```

{{content}}

]

--

**Issues**

- Assessing predictor's unique value 

- Add/drop terms substantially change the coefficients.

- Higher standard errors

- Model unstability

--

.pull-right[

**Variance inflation ratio (VIF)**

How much variance of a coefficients is increased due to multicollinearity.

$$\text{VIF} = \dfrac{1}{1-R^2_j}$$

where: $R^2_j$ is the $R^2$ obtained from regressing $X_j$ agains the rest of $X$s

> Rule of thumb: VIF > 10 might indicate serious multicollinearity. 

{{content}}

]

--

```{r}
vif(m)   # from car package
```

As expected, `length` and `diameter` have huge VIF.

Need to drop one of them o decorrelate them (tip: multivariate approach?)


???


---
name: model-building

## Building the model

.pull-left[

How to decide which variables to include?

- Context information about the problem

- Exploratory analysis: correlations, scatterplots

- Examining partial residuals (model mispecification)

How to evaluate models?

- Don't use $R^2$ as metric: the more predictors, the higher $R^2$.

- Use metrics that penalize for model complexity: AIC, BIC, Cp, Adj $R^2$

$$\text{Adj } R^2 = 1 - \dfrac{\SSErr/(n-p-1)}{\SSTot/(n-1)}$$

- Training/test or cross-validation approaches

]

???

Read

---
template: model-building

.pull-right[

There are automatic algorithms like _stepwise_ for variable sepection:

```{r}
m2 <- update(m, . ~ . -length)
m_step <- step(m2, scope = list(upper = ~ .^2))
```


]

???

After removing length to avoid colinearity with diameter, assuming diameter is easier to measure

We can run stepwise using step function which by default perform the procedure in both ways, forward and backwards. Optionally we can provide the scope limits. In this case, the first order interaction

In both direction algorithm each forward step is followed by a backwards selection to remove those terms that became not significant after addition. The step() function uses the AIC as criterion for comparing models.

In the example, it started by adding the interaction term and compared the new model with a model initial model (none) and a simpler model without hardeness + diameter.

After that it test the removal of this term. No further removal is tried because of the hierarchy principle: if an interaction is retained, also terms involved in the interaction must be present.


---
template: model-building

.pull-right[

```{r}
summary(m_step)
```

]

???

The summary of the model shows that the addition marginal improved the overall performance of the model (less residual standard error, higher R2)

---
## Model predictions

.pull-left[

Adding in-sample predictions

```{r}
preds_df <- predict(m_step, interval = "conf", se = T) |> 
  data.frame()
triticum <- bind_cols(triticum, preds_df)
```

Plotting predictions vs observed values

```{r obs-pred, echo = T, eval = F}
ggplot(triticum) +
  aes(x = weight, y = fit.fit) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  geom_abline(intercept = 0, slope = 1) +
  stat_poly_eq(use_label(c("eq", "R2"))) +
  labs(
    x = "predicted", y = "observed", 
    title = "Observed vs predicted"
  ) +
  theme_bw(base_size = 12)
```

]

.pull-right[

```{r, ref.label="obs-pred", out.width="80%", echo = F}
```

]

???

In order to evaluate how well the model predicts the response variable, we can make in sample predictions and compare them with the actual observed values. This approach is over-optimistc as the same information is being used for training and testing the model. 

A better approach to assess the predictive capacity of the model would include some form of independente validation.

Because the Better approaches would it is desirable to have a held-out set.

---
## Summary

We just learned...

- The importance of Multiple Linear Regression to model biological problems

- How to estimate a MLR model and make inferences

- How to interpret and assess variable contribution

- How to spot multicollinearity, a serious issue for model interpretation

- Ideas about model building

- How to get predictions from the model

--

.pull-right[

<br>

Next session: further extensions of MLR...

- Polynomial regression

- Dummy predictors

- Logistic regression

- More on model selection

- What to do when assumptions are not met?

]

???

Read

---
## Further readings

_Basic_


- Ott, L., and M. Longnecker. 2016. An introduction to statistical methods & data analysis. Seventh edition. Cengage Learning, Australia.

  Chapters 12 & 13

- Welham, S.J., S.A. Gezan, S.J. Clark, and A. Mead. 2014. Statistical Methods in Biology: Design and Analysis of Experiments and Regression. 1 edition. Chapman and Hall/CRC, Boca Raton.

  Chapters 8 & 9

- James, G., D. Witten, T. Hastie, and R. Tibshirani, editors. 2017. An introduction to statistical learning: with applications in R. 8th ed. Springer, New York.

  Chapter 3

_Advanced_

- Kutner, M., C. Nachtsheim, J. Neter, and W. Li. 2004. Applied Linear Statistical Models. 5th ed. McGraw-Hill/Irwin.

  Part II